{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore $TD(\\lambda)$ in the tabular setting, we'll be working with the (slightly modified) Frozen Lake task again; see the $\\texttt{Value Iteration and Policy Iteration}$ notebook for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'MDP' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-575d318b90cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Import MDP object for Frozen Lake environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FrozenLakeMDP.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'MDP' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "from frozen_lake import FrozenLakeEnv\n",
    "import pickle\n",
    "env = FrozenLakeEnv()\n",
    "#Import MDP object for Frozen Lake environment\n",
    "with open('FrozenLakeMDP.pkl', 'rb') as input:\n",
    "    mdp = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic imports and setup\n",
    "%matplotlib inline\n",
    "import numpy as np, numpy.random as nr, gym, copy\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 1 #Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(lambda) policy evaluation (NDP p.195 - 198)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the offline $TD(\\lambda)$ algorithm for policy evaluation, discussed on pages 195-198 of the NDP book.  We'll look at the differences between the fist- and every-visit methods, as well as the effects of varying $\\lambda$, as applied to the Frozen Lake task.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def offlineTDL(alpha, lambda_, mdp, nEp, pi, everyVisit=False):\n",
    "  '''  \n",
    "  Inputs\n",
    "    alpha: discount factor\n",
    "    lambda_: lambda from TD(lambda)\n",
    "    mdp: MDP object\n",
    "    nEp: number of episodes \n",
    "    pi: policy (function from s to a)\n",
    "    everyVisit: boolean for everyVisit method\n",
    "  Outputs\n",
    "    VList: list of estimated V-functions from each episode\n",
    "  '''\n",
    "  VList = []            \n",
    "  V = np.zeros(mdp.nS)  #Initialize value function \n",
    "  env = FrozenLakeEnv()\n",
    "  for ep in range(nEp):\n",
    "    visitDict = {s:np.array([]) for s in range(mdp.nS)} #Counts steps since each visit to state\n",
    "    Vupdate = np.zeros(mdp.nS)     #Accumulates TD errors at each state\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    while not done:         \n",
    "        #Get next action, state, reward, and TD error\n",
    "        a = pi(s)\n",
    "        sp, _, done, _ = env.step(a)\n",
    "        r = -0.001 * (sp != 15) + (sp == 15)\n",
    "        d = r + alpha*V[sp] - V[s]\n",
    "        \n",
    "        #Add visit if first visit, or using every visit method\n",
    "        if len(visitDict[s]) == 0 or everyVisit:\n",
    "          visitDict[s] = np.append(visitDict[s], 0)       \n",
    "        #Add to Vupdate \n",
    "        for s_ in range(mdp.nS):\n",
    "          Vupdate[s_] += np.sum(lambda_**visitDict[s_]) * d  \n",
    "          visitDict[s_] += np.ones(len(visitDict[s_]))\n",
    "        s = sp \n",
    "    V += (nEp+1)**(-0.75) * Vupdate  #Update V \n",
    "    VList.append(copy.copy(V))\n",
    "  return VList     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll test out the first- and every-visit variants of the algorithm, with varying values of $\\lambda$, by evaluating a policy on the Frozen Lake task.  We'll evaluate the optimal policy, found using the $\\texttt{policy_iteration}$ function I've defined in $\\texttt{policy_iteration.py}$.  \n",
    "\n",
    "After defining some functions to help us plot the V-function iterates at each episode, we run the algorithm for $\\lambda \\in \\{0, 0.5, 0.9, 1 \\}$ for both the first- and every-visit methods, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get policy to evaluate\n",
    "#  policy_iteration returns list of V-functions and optimal policies from \n",
    "#  each iteration\n",
    "from policy_iteration import policy_iteration\n",
    "VList_PI, policyList = policy_iteration(ALPHA, mdp, 20)\n",
    "pi = lambda s: policyList[-1][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def axisPlotter(ax, lambda_, VList): \n",
    "  title = 'Lambda = {}'.format(lambda_)    \n",
    "  ax.set_xlabel('Episode')\n",
    "  ax.set_ylabel('State value estimate')\n",
    "  ax.set_title(title)\n",
    "  ax.plot(VList)  \n",
    "\n",
    "def plotTDL(alpha, everyVisit, lambdas, nEp):\n",
    "  nlambda = len(lambdas)\n",
    "  everyVisitStr = ['First Visit', 'Every Visit']\n",
    "  print(everyVisitStr[everyVisit])\n",
    "  fig, axes = plt.subplots(1, 4, sharex=True, sharey=True)\n",
    "  for j in range(nlambda):\n",
    "    VList = offlineTDL(ALPHA, lambdas[j], mdp, nEp, pi, everyVisit=everyVisit)\n",
    "    ax = axes[j] \n",
    "    axisPlotter(ax, lambdas[j], VList)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALPHA = 1\n",
    "nEp = 10\n",
    "lambdas = [0, 0.5, 0.9, 1]\n",
    "plotTDL(ALPHA, False, lambdas, nEp)\n",
    "plotTDL(ALPHA, True,  lambdas, nEp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
