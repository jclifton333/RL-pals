{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore $TD(\\lambda)$ in the tabular setting, we'll be working with the (slightly modified) Frozen Lake task again; see the $\\texttt{Value Iteration and Policy Iteration}$ notebook for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from frozen_lake import FrozenLakeEnv\n",
    "import dill\n",
    "env = FrozenLakeEnv()\n",
    "#Import MDP object for Frozen Lake environment\n",
    "with open('FrozenLakeMDP.pkl', 'rb') as input:\n",
    "  mdp = dill.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic imports and setup\n",
    "%matplotlib inline\n",
    "import numpy as np, numpy.random as nr, gym, copy\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 1 #Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$) policy evaluation (NDP p.195 - 198)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the offline $TD(\\lambda)$ algorithm for policy evaluation, discussed on pages 195-198 of the NDP book.  We'll look at the differences between the fist- and every-visit methods, as well as the effects of varying $\\lambda$, as applied to the Frozen Lake task.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def offlineTDL(alpha, lambda_, nEp, pi, everyVisit=False, stepExponent=-0.75, stepMultiplier=1):\n",
    "  '''  \n",
    "  Inputs\n",
    "    alpha: discount factor\n",
    "    lambda_: lambda from TD(lambda)\n",
    "    mdp: MDP object\n",
    "    nEp: number of episodes \n",
    "    pi: policy (function from s to a)\n",
    "    everyVisit: boolean for everyVisit method\n",
    "    stepExponent and stepMultiplier: \n",
    "      step size = stepMultiplier * (nEp + 1)**stepExponent\n",
    "  Outputs\n",
    "    VList: list of estimated V-functions from each episode\n",
    "  '''\n",
    "  VList = []            \n",
    "  V = np.zeros(mdp.nS)  #Initialize value function \n",
    "  for ep in range(nEp):\n",
    "    visitDict = {s:np.array([]) for s in range(mdp.nS)} #Counts steps since each visit to state\n",
    "    Vupdate = np.zeros(mdp.nS)     #Accumulates TD errors at each state\n",
    "    s = 0 #Start at top-left\n",
    "    while True:     \n",
    "        #Get next action, state, reward, and TD error\n",
    "        a = pi(s)\n",
    "        sp, r = mdp.step(s, a)\n",
    "        d = r + alpha*V[sp] - V[s]\n",
    "        \n",
    "        #Add visit if first visit, or using every visit method\n",
    "        if len(visitDict[s]) == 0 or everyVisit:\n",
    "          visitDict[s] = np.append(visitDict[s], 0)       \n",
    "        #Add to Vupdate \n",
    "        for s_ in range(mdp.nS):\n",
    "          Vupdate[s_] += np.sum(lambda_**visitDict[s_]) * d  \n",
    "          visitDict[s_] += np.ones(len(visitDict[s_]))\n",
    "        s = sp \n",
    "        if s not in mdp.transient: \n",
    "          break \n",
    "    #Update V-function iterate\n",
    "    stepSize = stepMultiplier*(nEp + 1)**stepExponent\n",
    "    V += stepSize * Vupdate  \n",
    "    VList.append(copy.copy(V))\n",
    "  return np.array(VList)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll test out the first- and every-visit variants of the algorithm, with varying values of $\\lambda$, by evaluating a policy on the Frozen Lake task.  We'll evaluate the optimal policy, found using the $\\texttt{policy_iteration}$ function I've defined in $\\texttt{utils.py}$.  \n",
    "\n",
    "After defining some functions to help us plot the V-function iterates at each episode, we run the algorithm for $\\lambda \\in \\{0, 0.5, 0.9, 1 \\}$ for both the first- and every-visit methods, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MDP' object has no attribute 'optSoln'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-af57519bc075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#  each iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mVList_PI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicyList_PI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptSoln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpolicyList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MDP' object has no attribute 'optSoln'"
     ]
    }
   ],
   "source": [
    "#Get policy to evaluate\n",
    "#  policy_iteration returns list of V-functions and optimal policies from \n",
    "#  each iteration\n",
    "from utils import policy_iteration, plot_policy\n",
    "VList_PI, policyList_PI = mdp.policyIteration(alpha=ALPHA) \n",
    "pi = lambda s: policyList[-1][s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the optimal policy and corresponding state values look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_policy(env, VList_PI[-1:], policyList_PI[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(VList_PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def axisPlotter(ax, lambda_, VList): \n",
    "  title = 'Lambda = {}'.format(lambda_)    \n",
    "  ax.set_xlabel('Episode')\n",
    "  ax.set_ylabel('State value estimate')\n",
    "  ax.set_title(title)\n",
    "  for i in range(VList.shape[1]):\n",
    "    ax.plot(VList[:,i], label=i)  \n",
    "\n",
    "def plotVFs(alpha, everyVisit, lambdas, nEp, save=False):\n",
    "  nlambda = len(lambdas)\n",
    "  everyVisitStr = ['First Visit', 'Every Visit']\n",
    "  print(everyVisitStr[everyVisit])\n",
    "  fig, axes = plt.subplots(1, nlambda, sharex=True, sharey=True, figsize=(20,5))\n",
    "  for j in range(nlambda):\n",
    "    VList = offlineTDL(ALPHA, lambdas[j], nEp, pi, everyVisit=everyVisit)\n",
    "    ax = axes[j] \n",
    "    axisPlotter(ax, lambdas[j], VList)\n",
    "  ax.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0, title='State') \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 1\n",
    "nEp = 1000\n",
    "lambdas = [0, 0.5, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotVFs(ALPHA, False, lambdas, nEp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotVFs(ALPHA, True,  lambdas, nEp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
